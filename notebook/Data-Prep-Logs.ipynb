{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# All required imports for API Requeest of data base\n","import requests\n","import os\n","import json\n","from urllib3.exceptions import InsecureRequestWarning\n","from urllib3 import disable_warnings\n","\n","# All required imports for word extraction and analysis\n","import pandas as pd\n","from pandas import json_normalize\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","# All required imports for data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Get data from MongoDB Atlas Database"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#mongoDB api-key muss vorher in der variables.env hinterlgegt werden"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["disable_warnings(InsecureRequestWarning)\n","api_key = os.getenv('API_KEY')\n","\n","url = \"https://eu-central-1.aws.data.mongodb-api.com/app/data-lkdyd/endpoint/data/v1/action/find\"\n","payload = json.dumps({\n","    \"collection\": \"wartung-log-ausfall\",\n","    \"database\": \"data-project\",\n","    \"dataSource\": \"Cluster0\"\n","})\n","headers = {\n","  'Content-Type': 'application/json',\n","  'Access-Control-Request-Headers': '*',\n","  'api-key': api_key,\n","}\n","response = requests.request(\"POST\", url, headers=headers, data=payload, verify=False)\n","print(response.text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the response to a json object\n","json_data = json.loads(response.text)\n","json_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get response data as a dataframe\n","df = pd.DataFrame(json_data)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use json_normalize to flatten the dictionaries into separate columns\n","df_normalized = json_normalize(df['documents'])\n","\n","# Concatenate the normalized columns with the original DataFrame\n","df = pd.concat([df, df_normalized], axis=1)\n","\n","# Drop the original 'documents' column if needed\n","df = df.drop('documents', axis=1)\n","\n","# Print the resulting DataFrame\n","print(df)"]},{"cell_type":"markdown","metadata":{},"source":["Je nach Use Case hier noch JSON bearbeiten, bevor es in das DF überführt wird"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lower all the text in the dataframe\n","\n","df['LowText'] = df['LogMessage'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Data Preparation"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Text Preprocessing\n","- Tokenization\n","- Lemmatization\n","- Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('snowball_data')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Stemming\n","from nltk.stem.snowball import SnowballStemmer\n","snowball = SnowballStemmer(\"english\")\n","print(\"Stemming:\")\n","df['Stem'] = df['LowText'].apply(lambda x: ' '.join([snowball.stem(word) for word in str(x).split() if isinstance(x, str)]))\n","print(df['Stem'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#download tokenzization data\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tokenization\n","from nltk.tokenize import word_tokenize\n","print(\"Tokenization:\")\n","df['Token'] = [word_tokenize(word) for word in df[\"Stem\"]]\n","print(df['Token'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lemmatization in english language\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","print(\"--->Lemmatization:\")\n","df['Lem'] = [' '.join([lemmatizer.lemmatize(wd) for wd in word]) for word in df['Token']]\n","print(df['Lem'])"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Data Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check balance of ServiceOK and ServiceNotOK\n","print(df['ServiceOK'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use Smote to balance the data\n","from imblearn.over_sampling import SMOTE\n","from collections import Counter\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.pipeline import Pipeline\n","\n","# Define the resampling method\n","resampling = SMOTE(sampling_strategy='minority')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Analyse durchführen"]},{"cell_type":"markdown","metadata":{},"source":["Was müssen wir hier Analysieren? Reichen die Textdaten aus, oder müssen die Texte mit den anderen Hardware Daten verknüpft werden?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the data into training and test sets\n","from sklearn.model_selection import train_test_split\n","#Daten splitten\n","X_train, X_test, y_train, y_test = train_test_split(df['Stem'], df['ServiceOK'],\n","test_size=0.2, random_state=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Vectorization of the Data\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n","tfidf_test_vectors = tfidf_vectorizer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use SMOTE to balance the data\n","from imblearn.over_sampling import SMOTE\n","\n","# Define the resampling method\n","smote = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = smote.fit_resample(tfidf_train_vectors , y_train)\n","\n","print('Original dataset shape %s' % Counter(y_train_resampled))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the data\n","\n","def plot_confusion_matrix(y_true, y_pred, title, labels):\n","    cm = confusion_matrix(y_true, y_pred)\n","    fig, ax = plt.subplots(figsize=(8, 8))\n","    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n","    ax.set_xlabel('Predicted', fontsize=15)\n","    ax.set_ylabel('True', fontsize=15)\n","    ax.set_title(title, fontsize=15)\n","    ax.xaxis.set_ticklabels(labels)\n","    ax.yaxis.set_ticklabels(labels, rotation=0)\n","    plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 Klassifikation - K-nearest Neighbors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# K-nearest neighbors\n","from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier(n_neighbors=3)\n","knn.fit(X_train_resampled, y_train_resampled)\n","knn_pred = knn.predict(tfidf_test_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Classification report\n","print(\"K-nearest neighbors:\\n\", classification_report(y_test, knn_pred))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_confusion_matrix(y_test, knn_pred, \"K-nearest neighbors\", knn.classes_)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 Klassifikation - Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Random Forest\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=0)\n","rf.fit(tfidf_train_vectors, y_train)\n","rf_pred = rf.predict(tfidf_test_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Classification report\n","print(\"Random Forest:\\n\", classification_report(y_test, rf_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create Decision Tree for the prediction of the next value of ServiceOK\n","plot_confusion_matrix(y_test, rf_pred, \"Random Forrest\", rf.classes_)\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":2}
